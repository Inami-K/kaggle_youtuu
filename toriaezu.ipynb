{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, sys, copy, pickle\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import timm\n",
    "\n",
    "import cv2\n",
    "cv2.setNumThreads(0)\n",
    "import PIL\n",
    "import pydicom\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "#     os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n",
    "#     tf.random.set_seed(SEED)\n",
    "#     keras.utils.set_random_seed(seed=SEED)\n",
    "    print('seeding done!!!')\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この設定は、PL-RSNA-2024-Lumbar-Spine-Classificationというプロジェクトで使用するモデルの設定ですね。以下に設定の詳細を説明します：\n",
    "\n",
    "project_name: プロジェクトの名前です。Lumbar Spineの分類に関連しているようです。\n",
    "\n",
    "artifact_name: モデルのアーティファクト名です。\n",
    "\n",
    "load_kernel: カーネルの読み込み設定ですが、現在はNoneです。\n",
    "\n",
    "load_last: 最後の重みを読み込むかどうかを示すフラグで、Trueに設定されています。\n",
    "\n",
    "n_folds: クロスバリデーションの分割数です。ここでは5に設定されています。\n",
    "\n",
    "backbone: 使用するバックボーンモデルの指定です。efficientnet_b0.ra_in1kが使われています。\n",
    "\n",
    "img_size: 入力画像のサイズです。384x384ピクセルに設定されています。\n",
    "\n",
    "n_slice_per_c: 1つのコンポーネントあたりのスライス数です。16に設定されています。\n",
    "\n",
    "in_chans: 入力チャンネル数です。ここでは1に設定されています（グレースケール画像を想定）。\n",
    "\n",
    "drop_rate: ドロップアウト率です。0に設定されています。\n",
    "\n",
    "drop_rate_last: 最終層のドロップアウト率です。0.3に設定されています。\n",
    "\n",
    "drop_path_rate: ドロップパスの割合です。0に設定されています。\n",
    "\n",
    "p_mixup: Mixupの確率です。0.5に設定されています。\n",
    "\n",
    "p_rand_order_v1: ランダムオーダーの確率です。0.2に設定されています。\n",
    "\n",
    "lr: 学習率です。1e-3 (0.001)に設定されています。\n",
    "\n",
    "out_dim: 出力次元数です。3に設定されています（3クラス分類を想定）。\n",
    "\n",
    "epochs: 学習エポック数です。15に設定されています。\n",
    "\n",
    "batch_size: バッチサイズです。8に設定されています。\n",
    "\n",
    "device: 使用するデバイスです。CUDAが利用可能な場合はGPU、それ以外の場合はCPUを指定しています。\n",
    "\n",
    "seed: 乱数シードです。2024に設定されています。\n",
    "\n",
    "これらの設定は、モデルのアーキテクチャや学習方法、データの扱い方などを決定するための重要なパラメータです。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeding done!!!\n"
     ]
    }
   ],
   "source": [
    "CONFIG = dict(\n",
    "    project_name = \"PL-RSNA-2024-Lumbar-Spine-Classification\",\n",
    "    artifact_name = \"rsnaEffNetModel\",\n",
    "    load_kernel = None,\n",
    "    load_last = True,\n",
    "    n_folds = 5,\n",
    "    backbone = \"efficientnet_b0.ra_in1k\", # tf_efficientnetv2_s_in21ft1k\n",
    "    img_size = 384,\n",
    "    n_slice_per_c = 16,\n",
    "    in_chans = 1,\n",
    "\n",
    "    drop_rate = 0.,\n",
    "    drop_rate_last = 0.3,\n",
    "    drop_path_rate = 0.,\n",
    "    p_mixup = 0.5,\n",
    "    p_rand_order_v1 = 0.2,\n",
    "    lr = 1e-3,\n",
    "\n",
    "    out_dim = 3,\n",
    "    epochs = 15,\n",
    "    batch_size = 4, \n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\",\n",
    "    seed = 2024\n",
    ")\n",
    "\n",
    "seeding(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この設定（CONFIG）は、ある特定のプロジェクト（PL-RSNA-2024-Lumbar-Spine-Classification）でEfficientNetモデルを使用して腰椎の分類を行うためのパラメータを定義しています。\n",
    "\n",
    "project_name: プロジェクトの名前を示しています。\n",
    "\n",
    "artifact_name: モデルの保存時に使用するアーティファクト名です。\n",
    "\n",
    "load_kernel: カーネルの読み込み設定ですが、Noneに設定されています。\n",
    "\n",
    "load_last: 最後の重みを読み込むかどうかを示すフラグで、Trueに設定されています。\n",
    "\n",
    "n_folds: クロスバリデーションの分割数です。ここでは5に設定されています。\n",
    "\n",
    "backbone: 使用するバックボーンモデルの指定です。\"efficientnet_b0.ra_in1k\"が使用されています。\n",
    "\n",
    "img_size: 入力画像のサイズです。384x384ピクセルに設定されています。\n",
    "\n",
    "n_slice_per_c: 1つのコンポーネントあたりのスライス数です。ここでは16に設定されています。\n",
    "\n",
    "in_chans: 入力チャンネル数です。1に設定されています（グレースケール画像を想定）。\n",
    "\n",
    "drop_rate: ドロップアウト率です。0に設定されています。\n",
    "\n",
    "drop_rate_last: 最終層のドロップアウト率です。0.3に設定されています。\n",
    "\n",
    "drop_path_rate: ドロップパスの割合です。0に設定されています。\n",
    "\n",
    "p_mixup: Mixupの確率です。0.5に設定されています。\n",
    "\n",
    "p_rand_order_v1: ランダムオーダーの確率です。0.2に設定されています。\n",
    "\n",
    "lr: 学習率です。1e-3 (0.001)に設定されています。\n",
    "\n",
    "out_dim: 出力次元数です。3に設定されています（3クラス分類を想定）。\n",
    "\n",
    "epochs: 学習エポック数です。15に設定されています。\n",
    "\n",
    "batch_size: バッチサイズです。8に設定されています。\n",
    "\n",
    "device: 使用するデバイスです。CUDAが利用可能な場合はGPU、それ以外の場合はCPUが選択されます。\n",
    "\n",
    "seed: 乱数シードです。2024に設定されています。\n",
    "\n",
    "これらの設定は、モデルのアーキテクチャ、学習方法、データの処理方法などを制御し、特定のタスクに最適化されたモデルのトレーニングを可能にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv('DATA_PATH/sample_submission.csv')\n",
    "test_desc = pd.read_csv('DATA_PATH/test_series_descriptions.csv')\n",
    "train_desc = pd.read_csv('DATA_PATH/train_series_descriptions.csv')\n",
    "train_main = pd.read_csv('DATA_PATH/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>image_path</th>\n",
       "      <th>condition</th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>test_imgs\\44036939\\2828203845\\1.dcm</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>test_imgs\\44036939\\2828203845\\10.dcm</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>test_imgs\\44036939\\2828203845\\11.dcm</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>test_imgs\\44036939\\2828203845\\12.dcm</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>test_imgs\\44036939\\2828203845\\13.dcm</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id   series_id series_description  \\\n",
       "0  44036939  2828203845        Sagittal T1   \n",
       "1  44036939  2828203845        Sagittal T1   \n",
       "2  44036939  2828203845        Sagittal T1   \n",
       "3  44036939  2828203845        Sagittal T1   \n",
       "4  44036939  2828203845        Sagittal T1   \n",
       "\n",
       "                             image_path                        condition  \\\n",
       "0   test_imgs\\44036939\\2828203845\\1.dcm  left_neural_foraminal_narrowing   \n",
       "1  test_imgs\\44036939\\2828203845\\10.dcm  left_neural_foraminal_narrowing   \n",
       "2  test_imgs\\44036939\\2828203845\\11.dcm  left_neural_foraminal_narrowing   \n",
       "3  test_imgs\\44036939\\2828203845\\12.dcm  left_neural_foraminal_narrowing   \n",
       "4  test_imgs\\44036939\\2828203845\\13.dcm  left_neural_foraminal_narrowing   \n",
       "\n",
       "                                     row_id  target  \n",
       "0  44036939_left_neural_foraminal_narrowing       0  \n",
       "1  44036939_left_neural_foraminal_narrowing       0  \n",
       "2  44036939_left_neural_foraminal_narrowing       0  \n",
       "3  44036939_left_neural_foraminal_narrowing       0  \n",
       "4  44036939_left_neural_foraminal_narrowing       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the base path for test images\n",
    "base_path = f'test_imgs'\n",
    "\n",
    "# function to get image paths for a series\n",
    "def get_image_paths(row):\n",
    "    series_path = os.path.join(base_path, str(row['study_id']), str(row['series_id']))\n",
    "    if os.path.exists(series_path):\n",
    "        return [\n",
    "            os.path.join(series_path, f) for f in os.listdir(series_path) if os.path.isfile(os.path.join(series_path, f))\n",
    "        ]\n",
    "    return []\n",
    "\n",
    "# Mapping of series_description to conditions\n",
    "condition_mapping = {\n",
    "    'Sagittal T1': {'left': 'left_neural_foraminal_narrowing', 'right': 'right_neural_foraminal_narrowing'},\n",
    "    'Axial T2': {'left': 'left_subarticular_stenosis', 'right': 'right_subarticular_stenosis'},\n",
    "    'Sagittal T2/STIR': 'spinal_canal_stenosis'\n",
    "}\n",
    "\n",
    "# Create a list to store the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Expand the dataframe by adding new rows for each file path\n",
    "for index, row in test_desc.iterrows():\n",
    "    image_paths = get_image_paths(row)\n",
    "    conditions = condition_mapping.get(row['series_description'], {})\n",
    "    if isinstance(conditions, str):  # Single condition\n",
    "        conditions = {'left': conditions, 'right': conditions}\n",
    "    for side, condition in conditions.items():\n",
    "        for image_path in image_paths:\n",
    "            expanded_rows.append({\n",
    "                'study_id': row['study_id'],\n",
    "                'series_id': row['series_id'],\n",
    "                'series_description': row['series_description'],\n",
    "                'image_path': image_path,\n",
    "                'condition': condition,\n",
    "                'row_id': f\"{row['study_id']}_{condition}\"\n",
    "            })\n",
    "\n",
    "# Create a new dataframe from the expanded rows\n",
    "expanded_test_desc = pd.DataFrame(expanded_rows)\n",
    "\n",
    "test_data = expanded_test_desc.copy()\n",
    "test_data['target'] = 0\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"Normal/Mild\": 0, \"Moderate\": 1, \"Severe\": 2}\n",
    "id2label = {v:k for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom(path):\n",
    "    dicom = pydicom.read_file(path)\n",
    "    data = dicom.pixel_array\n",
    "    data = data - np.min(data)\n",
    "    if np.max(data) != 0:\n",
    "        data = data / np.max(data)\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このload_dicom関数は、DICOM形式の画像ファイルを読み込み、適切な形式で処理しています。以下に関数の詳細を説明します。\n",
    "\n",
    "load_dicom(path) 関数の詳細\n",
    "関数の目的:\n",
    "\n",
    "DICOM形式の画像ファイルを指定されたパスから読み込みます。\n",
    "読み込んだDICOM画像データを適切な形式に変換して返します。\n",
    "引数:\n",
    "\n",
    "path (str): 読み込むDICOMファイルのパス。\n",
    "処理内容:\n",
    "\n",
    "pydicom.read_file(path): pydicomライブラリを使用して指定されたパスのDICOMファイルを読み込みます。dicomオブジェクトとして取得します。\n",
    "\n",
    "dicom.pixel_array: DICOM画像のピクセルデータを取得します。これはNumPy配列として表されます。\n",
    "\n",
    "data = data - np.min(data): ピクセルデータを最小値で補正し、負の値をゼロにします。これにより、データの範囲が0以上になります。\n",
    "\n",
    "if np.max(data) != 0:: 最大値が0でない場合、データを最大値で正規化します。これにより、データが0から1の範囲にスケーリングされます。\n",
    "\n",
    "data = (data * 255).astype(np.uint8): 画像データを8ビット符号なし整数 (uint8) に変換し、0から255の範囲にスケーリングします。これにより、画像データをグレースケールの8ビット符号なし整数形式で表現できます。\n",
    "\n",
    "return data: 変換されたDICOM画像データを返します。\n",
    "\n",
    "使用するライブラリ\n",
    "pydicom: DICOMファイルを読み込むためのライブラリです。DICOMファイルには患者の医療画像や関連情報が含まれています。\n",
    "\n",
    "numpy (npとしてインポート): 数値計算を行うための基本的なライブラリです。画像データの処理や変換に使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, label_name='target'):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.label = dataframe.loc[:, label_name]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.dataframe['image_path'][index]\n",
    "        image = load_dicom(image_path)  # Define this function to load your DICOM images\n",
    "        target = self.dataframe['target'][index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "            image = self.transform(image=image)['image']\n",
    "            image = image.transpose(2, 0, 1).astype(np.float32) / 255.\n",
    "\n",
    "        return image, torch.tensor(target).float()\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__init__ メソッド:\n",
    "\n",
    "dataframe: 入力データを保持するPandas DataFrame。\n",
    "transform: 前処理を実行するためのTransform関数（例えば、画像のリサイズや正規化など）。\n",
    "label_name: ラベル列の名前。デフォルトは 'target' です。\n",
    "self.dataframe: 入力データを保持するDataFrameをクラスのインスタンス変数として格納します。\n",
    "self.transform: 前処理関数が与えられていれば、そのままインスタンス変数として格納します。\n",
    "self.label: 指定されたラベル列を取得し、self.label として保持します。\n",
    "\n",
    "__len__ メソッド:\n",
    "\n",
    "データセットのサイズ（データ数）を返します。\n",
    "\n",
    "__getitem__ メソッド:\n",
    "\n",
    "指定されたインデックス index に対応するデータを返します。\n",
    "DICOM画像のパスを取得し、load_dicom 関数を使ってDICOM画像を読み込みます。\n",
    "データフレームからターゲットラベルを取得します。\n",
    "self.transform が指定されている場合は、画像に対して指定された前処理を適用します（ここではOpenCVを使用してグレースケールからBGRに変換し、指定されたTransform関数を適用しています）。\n",
    "画像データをPyTorchのテンソルに変換して返し、対応するターゲットラベルも返します。\n",
    "\n",
    "get_labels メソッド:\n",
    "\n",
    "データセット全体のラベルを返します。\n",
    "\n",
    "使用するライブラリ\n",
    "torch: PyTorchライブラリ。\n",
    "torchvision: PyTorchの画像処理用ライブラリ。\n",
    "cv2 (OpenCV): 画像の読み込み、変換に使用されます。\n",
    "numpy (npとしてインポート): 数値計算を行うための基本的なライブラリ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(height, width):\n",
    "    train_tsfm = A.Compose([\n",
    "        # Geometric augmentations\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(limit=(-30, 30), p=0.5),  # 回転の角度の範囲をタプルで指定する\n",
    "        \n",
    "        A.Resize(height=height, width=width),\n",
    "    ])\n",
    "    \n",
    "    valid_tsfm = A.Compose([\n",
    "        A.Resize(height=height, width=width),\n",
    "    ])\n",
    "    \n",
    "    return {\"train\": train_tsfm, \"eval\": valid_tsfm}\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloaders(data, cfg, split=\"train\"):\n",
    "    img_size = cfg['img_size']\n",
    "    height, width = img_size, img_size\n",
    "    tsfm = get_transforms(height=height, width=width)\n",
    "    if split == 'train':\n",
    "        tr_tsfm = tsfm['train']\n",
    "        ds = CustomDataset(data, transform=tr_tsfm)\n",
    "        labels = ds.get_labels()\n",
    "#         class_weights = torch.tensor(compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels))\n",
    "        class_weights = torch.tensor([1, 2, 4])\n",
    "        samples_weights = class_weights[labels]\n",
    "#         print(class_weights)\n",
    "        sampler = WeightedRandomSampler(weights=samples_weights, \n",
    "                                        num_samples=len(samples_weights), \n",
    "                                        replacement=True)\n",
    "\n",
    "        dls = DataLoader(ds, \n",
    "                         batch_size=cfg['batch_size'], \n",
    "                         sampler=sampler, \n",
    "                         num_workers=os.cpu_count(), \n",
    "                         drop_last=True, \n",
    "                         pin_memory=True)\n",
    "        \n",
    "    elif split == 'valid' or split == 'test':\n",
    "        eval_tsfm = tsfm['eval']\n",
    "        ds = CustomDataset(data, transform=eval_tsfm)\n",
    "        dls = DataLoader(ds, \n",
    "                         batch_size=2*cfg['batch_size'], \n",
    "                         shuffle=False, \n",
    "                         num_workers=os.cpu_count(), \n",
    "                         drop_last=False, \n",
    "                         pin_memory=True)\n",
    "    else:\n",
    "        raise Exception(\"Split should be 'train' or 'valid' or 'test'!!!\")\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは、データセットのトランスフォームやデータローダーを作成する関数を定義しています。以下に関数の詳細を説明します。\n",
    "\n",
    "get_transforms(height, width): 画像のトランスフォーメーションを定義する関数です。\n",
    "\n",
    "train_tsfm: 学習時のデータ拡張（Augmentation）を定義します。水平方向と垂直方向のフリップ、ランダムな角度での回転、指定されたサイズへのリサイズを行います。\n",
    "\n",
    "valid_tsfm: 検証時およびテスト時のデータセットのリサイズを行うトランスフォーメーションです。\n",
    "\n",
    "A.Compose([...]): albumentationsライブラリを使用して複数の画像変換を組み合わせています。これにより、データセットの多様性を増やし、モデルの汎化性能を向上させます。\n",
    "\n",
    "\n",
    "get_dataloaders(data, cfg, split=\"train\"): データセットをロードするための関数です。指定されたデータを元に、学習用、検証用、またはテスト用のデータローダーを作成します。\n",
    "\n",
    "split: 'train', 'valid', 'test' のいずれかで、データセットの使用目的を指定します。\n",
    "\n",
    "tr_tsfmやeval_tsfm: get_transforms 関数で定義したトランスフォーメーションを取得します。\n",
    "\n",
    "CustomDataset(data, transform=tr_tsfm): CustomDataset クラスを使用して、指定されたデータとトランスフォーメーションを適用したデータセットを作成します。\n",
    "\n",
    "WeightedRandomSampler: 各クラスのサンプル数に基づいて重み付けされたランダムサンプリングを行うためのサンプラーです。クラスの不均衡を考慮して学習を効果的に行うために使用されます。\n",
    "\n",
    "DataLoader: PyTorchのデータローダーを作成し、ミニバッチでデータを読み込みます。ここでは、並列処理を行うためにnum_workersをCPUのコア数として指定し、pin_memory=Trueでメモリピンを有効にしています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmModel(nn.Module):\n",
    "    def __init__(self, backbone, pretrained=False):\n",
    "        super(TimmModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            num_classes=CONFIG[\"out_dim\"],\n",
    "            features_only=False,\n",
    "            drop_rate=CONFIG[\"drop_rate\"],\n",
    "            drop_path_rate=CONFIG[\"drop_path_rate\"],\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=CONFIG[\"drop_rate\"], bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(CONFIG[\"drop_rate_last\"]),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, CONFIG[\"out_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        feat, _ = self.lstm(feat)\n",
    "        feat = self.head(feat)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLIPS = [None, [-1], [-2], [-2, -1]]\n",
    "\n",
    "def inference_loop(model, loader):\n",
    "    model.to(CONFIG[\"device\"])\n",
    "    model.eval()\n",
    "    preds = np.empty((0, 3))\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            images, labels = batch\n",
    "            images = images.to(CONFIG[\"device\"], non_blocking=True)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "#                 logits = model(images.to(torch.float32))\n",
    "                logits = model(images)\n",
    "#                 logits = logits.mean(axis=1).softmax(dim=-1)\n",
    "                logits = logits.softmax(dim=-1)\n",
    "                preds = np.concatenate([preds, logits.detach().cpu().numpy()])\n",
    "    np.save('preds.npy', preds)\n",
    "    \n",
    "    \n",
    "def tta_inference_loop(model, loader):\n",
    "    model.to(CONFIG[\"device\"])\n",
    "    model.eval()\n",
    "    preds = np.empty((0, 3))\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            images, labels = batch\n",
    "            images = images.to(CONFIG[\"device\"], non_blocking=True)\n",
    "            pred_tta = []\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                for f in FLIPS:\n",
    "                    logits = model(torch.flip(images, f) if f is not None else images)\n",
    "                    logits = logits.softmax(dim=-1)\n",
    "                    pred_tta.append(logits.detach().cpu().numpy())\n",
    "#                 preds = np.concatenate([preds, logits.detach().cpu().numpy()])\n",
    "                preds = np.concatenate([preds, np.mean(pred_tta, 0)])\n",
    "    np.save('preds.npy', preds)\n",
    "#     return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference_loop(model, loader) 関数\n",
    "\n",
    "model.to(CONFIG[\"device\"]): モデルをGPU（cuda）またはCPUに移動します。CONFIG[\"device\"]は事前に定義されたデバイスです。\n",
    "\n",
    "model.eval(): モデルを評価モードに設定します。これにより、ドロップアウトやバッチ正規化などの層の挙動がテストモードとなります。\n",
    "\n",
    "torch.no_grad(): 勾配計算を無効にします。推論時には勾配は不要なため、メモリを節約します。\n",
    "\n",
    "for batch in tqdm(loader):: データローダーからバッチごとにデータを取得します。tqdmは進捗バーを表示するためのライブラリです。\n",
    "\n",
    "images, labels = batch: バッチから画像データと対応するラベルを取得します。\n",
    "\n",
    "images.to(CONFIG[\"device\"], non_blocking=True): 画像データをGPUに移動します。non_blocking=Trueは非同期処理を意味し、メモリの効率を向上させます。\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):: 半精度での推論を行います。これにより、計算速度が向上し、GPUメモリの使用量が削減されます。\n",
    "\n",
    "logits = model(images): モデルに画像データを入力し、ロジット（未活性化の出力）を取得します。\n",
    "\n",
    "logits.softmax(dim=-1): ロジットをソフトマックス関数で確率に変換します。dim=-1は最後の次元でソフトマックスを計算することを意味します。\n",
    "\n",
    "preds = np.concatenate([preds, logits.detach().cpu().numpy()]): ロジットをnumpy配列に変換し、predsに追加します。detach()はTensorから計算グラフを切り離し、cpu().numpy()でCPU上のnumpy配列に変換します。\n",
    "\n",
    "np.save('preds.npy', preds): 予測結果をpreds.npyというファイルに保存します。\n",
    "\n",
    "tta_inference_loop(model, loader) 関数\n",
    "\n",
    "for f in FLIPS:: TTA（Test Time Augmentation）の各操作についてループします。FLIPSは水平、垂直方向へのフリップの組み合わせを示すリストです。\n",
    "\n",
    "torch.flip(images, f): 画像データにTTAを適用します。fがNoneでない場合は、指定された軸で画像をフリップします。\n",
    "\n",
    "pred_tta.append(logits.detach().cpu().numpy()): TTAで得られた予測結果をリスト pred_tta に追加します。\n",
    "\n",
    "preds = np.concatenate([preds, np.mean(pred_tta, 0)]): TTAで得られた各予測結果の平均を取り、predsに追加します。これにより、複数のデータ拡張による予測を平均してアンサンブル効果を得ます。\n",
    "\n",
    "これらの関数を使えば、モデルの推論を効率的に行い、さまざまなデータ拡張技術を組み合わせて精度向上を図ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_path = \"model_weights.pth\"\n",
    "weights = torch.load(weights_path, map_location=torch.device(\"cpu\"))\n",
    "model = TimmModel(backbone=CONFIG[\"backbone\"], pretrained=False)\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoaderを直接初期化し、num_workersを0に設定する\n",
    "dataloader = DataLoader(dataset=test_data, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed66129d2294516828a414633ee8695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 3680, 4652, 7900, 12644, 9884, 1536, 1776, 8916, 8952, 9396, 5448, 9280) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dls \u001b[38;5;241m=\u001b[39m get_dataloaders(test_data, CONFIG, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# inference_loop(model, dls)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtta_inference_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# _ = Parallel(n_jobs=mp.cpu_count())(\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     delayed(inference_loop(model, dls))\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m      8\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 25\u001b[0m, in \u001b[0;36mtta_inference_loop\u001b[1;34m(model, loader)\u001b[0m\n\u001b[0;32m     23\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 3680, 4652, 7900, 12644, 9884, 1536, 1776, 8916, 8952, 9396, 5448, 9280) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "dls = get_dataloaders(test_data, CONFIG, split=\"test\")\n",
    "# inference_loop(model, dls)\n",
    "tta_inference_loop(model, dls)\n",
    "# _ = Parallel(n_jobs=mp.cpu_count())(\n",
    "#     delayed(inference_loop(model, dls))\n",
    "# )\n",
    "\n",
    "preds = np.load('preds.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
